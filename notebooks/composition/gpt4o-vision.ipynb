{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "IMAGE_PATH = \"data/Assets/0a22f881b77f00220f2034c21a18b854/_preview.png\"\n",
    "\n",
    "# Open the image file and encode it as a base64 string\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "base64_image = encode_image(IMAGE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iVBORw0KGgoAAAANSUhEUgAAAlgAAAH0CAMAAADWjqPmAAADAFBMVEUICAwLDREICQ4MDRIAAAAKDBAKCg8JCQ7///8JCw8NDxIJ...hFR1wLRn4QUeIbUHnQ/pDVa4AB14g+d2fLOygoJmzgwPD9+CADOBnGwQKAOBzExxKFiyZg0AMxoTpETVicUAAAAASUVORK5CYII=\n"
     ]
    }
   ],
   "source": [
    "print(base64_image[:100] + \"...\" + base64_image[-100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "## Set the API key\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image you provided contains several components, including text, logos, and a photograph of a camera lens. To perform image analysis such as detecting objects, colors, and text, you can use various computer vision tools and libraries. Here are some suggestions:\n",
      "\n",
      "### Tools and Libraries\n",
      "\n",
      "1. **Pillow**: A Python Imaging Library (PIL) fork that is easy to use for basic image processing tasks.\n",
      "2. **OpenCV**: A powerful library for computer vision tasks, including object detection, image processing, and more.\n",
      "3. **TensorFlow**: A machine learning library that can be used for more advanced tasks like object detection and image classification.\n",
      "4. **Tesseract**: An OCR (Optical Character Recognition) engine that can be used to extract text from images.\n",
      "\n",
      "### Steps for Image Analysis\n",
      "\n",
      "#### 1. Detecting Objects\n",
      "To detect objects in the image, you can use pre-trained models available in TensorFlow or OpenCV.\n",
      "\n",
      "**Using OpenCV and TensorFlow:**\n",
      "```python\n",
      "import cv2\n",
      "import numpy as np\n",
      "import tensorflow as tf\n",
      "\n",
      "# Load the image\n",
      "image = cv2.imread('image_path.jpg')\n",
      "\n",
      "# Load a pre-trained model (e.g., MobileNet SSD)\n",
      "model = tf.saved_model.load('ssd_mobilenet_v2_fpnlite_320x320/saved_model')\n",
      "\n",
      "# Preprocess the image\n",
      "input_tensor = tf.convert_to_tensor(image)\n",
      "input_tensor = input_tensor[tf.newaxis, ...]\n",
      "\n",
      "# Perform object detection\n",
      "detections = model(input_tensor)\n",
      "\n",
      "# Process the detections\n",
      "for detection in detections['detection_boxes']:\n",
      "    # Draw bounding boxes, etc.\n",
      "    pass\n",
      "\n",
      "# Display the image\n",
      "cv2.imshow('Image', image)\n",
      "cv2.waitKey(0)\n",
      "cv2.destroyAllWindows()\n",
      "```\n",
      "\n",
      "#### 2. Detecting Colors\n",
      "To detect colors in the image, you can use OpenCV to convert the image to different color spaces and analyze the pixel values.\n",
      "\n",
      "**Using OpenCV:**\n",
      "```python\n",
      "import cv2\n",
      "\n",
      "# Load the image\n",
      "image = cv2.imread('image_path.jpg')\n",
      "\n",
      "# Convert to HSV color space\n",
      "hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
      "\n",
      "# Define color ranges and create masks\n",
      "lower_red = np.array([0, 120, 70])\n",
      "upper_red = np.array([10, 255, 255])\n",
      "mask = cv2.inRange(hsv_image, lower_red, upper_red)\n",
      "\n",
      "# Apply the mask to the image\n",
      "result = cv2.bitwise_and(image, image, mask=mask)\n",
      "\n",
      "# Display the result\n",
      "cv2.imshow('Detected Colors', result)\n",
      "cv2.waitKey(0)\n",
      "cv2.destroyAllWindows()\n",
      "```\n",
      "\n",
      "#### 3. Extracting Text\n",
      "To extract text from the image, you can use Tesseract OCR.\n",
      "\n",
      "**Using Tesseract:**\n",
      "```python\n",
      "from PIL import Image\n",
      "import pytesseract\n",
      "\n",
      "# Load the image\n",
      "image = Image.open('image_path.jpg')\n",
      "\n",
      "# Perform OCR\n",
      "text = pytesseract.image_to_string(image)\n",
      "\n",
      "print(text)\n",
      "```\n",
      "\n",
      "### Example Analysis of the Provided Image\n",
      "\n",
      "1. **Text Detection**: Use Tesseract to extract the text \"Lexus\", \"L/Certified by Lexus\", \"Tap the screen to find the nearest Lexus dealership\".\n",
      "2. **Object Detection**: Use TensorFlow or OpenCV to detect the camera lens and the hand icon.\n",
      "3. **Color Detection**: Use OpenCV to identify the dominant colors in the image, such as black, white, and the colors on the camera lens.\n",
      "\n",
      "By combining these tools and techniques, you can perform comprehensive image analysis on the provided image.\n"
     ]
    }
   ],
   "source": [
    "MODEL=\"gpt-4o\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful code and programming assistant. Help me with the programming and code problems.\"},\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"What's the components of this image and how do I perform image analysis, like detecting objects, colors, text, etc.? Suggest tools to use are Computer Vision like pillow, opencv, tensorflow, etc.\"},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\n",
    "                \"url\": f\"data:image/png;base64,{base64_image}\"}\n",
    "            }\n",
    "        ]}\n",
    "    ],\n",
    "    temperature=0.0,\n",
    ")\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-9k4ZQJ1mky3XenmVFKTHtp2GgQMTY', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The image you provided contains several components, including text, logos, and a photograph of a camera lens. To perform image analysis such as detecting objects, colors, and text, you can use various computer vision tools and libraries. Here are some suggestions:\\n\\n### Tools and Libraries\\n\\n1. **Pillow**: A Python Imaging Library (PIL) fork that is easy to use for basic image processing tasks.\\n2. **OpenCV**: A powerful library for computer vision tasks, including object detection, image processing, and more.\\n3. **TensorFlow**: A machine learning library that can be used for more advanced tasks like object detection and image classification.\\n4. **Tesseract**: An OCR (Optical Character Recognition) engine that can be used to extract text from images.\\n\\n### Steps for Image Analysis\\n\\n#### 1. Detecting Objects\\nTo detect objects in the image, you can use pre-trained models available in TensorFlow or OpenCV.\\n\\n**Using OpenCV and TensorFlow:**\\n```python\\nimport cv2\\nimport numpy as np\\nimport tensorflow as tf\\n\\n# Load the image\\nimage = cv2.imread(\\'image_path.jpg\\')\\n\\n# Load a pre-trained model (e.g., MobileNet SSD)\\nmodel = tf.saved_model.load(\\'ssd_mobilenet_v2_fpnlite_320x320/saved_model\\')\\n\\n# Preprocess the image\\ninput_tensor = tf.convert_to_tensor(image)\\ninput_tensor = input_tensor[tf.newaxis, ...]\\n\\n# Perform object detection\\ndetections = model(input_tensor)\\n\\n# Process the detections\\nfor detection in detections[\\'detection_boxes\\']:\\n    # Draw bounding boxes, etc.\\n    pass\\n\\n# Display the image\\ncv2.imshow(\\'Image\\', image)\\ncv2.waitKey(0)\\ncv2.destroyAllWindows()\\n```\\n\\n#### 2. Detecting Colors\\nTo detect colors in the image, you can use OpenCV to convert the image to different color spaces and analyze the pixel values.\\n\\n**Using OpenCV:**\\n```python\\nimport cv2\\n\\n# Load the image\\nimage = cv2.imread(\\'image_path.jpg\\')\\n\\n# Convert to HSV color space\\nhsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\\n\\n# Define color ranges and create masks\\nlower_red = np.array([0, 120, 70])\\nupper_red = np.array([10, 255, 255])\\nmask = cv2.inRange(hsv_image, lower_red, upper_red)\\n\\n# Apply the mask to the image\\nresult = cv2.bitwise_and(image, image, mask=mask)\\n\\n# Display the result\\ncv2.imshow(\\'Detected Colors\\', result)\\ncv2.waitKey(0)\\ncv2.destroyAllWindows()\\n```\\n\\n#### 3. Extracting Text\\nTo extract text from the image, you can use Tesseract OCR.\\n\\n**Using Tesseract:**\\n```python\\nfrom PIL import Image\\nimport pytesseract\\n\\n# Load the image\\nimage = Image.open(\\'image_path.jpg\\')\\n\\n# Perform OCR\\ntext = pytesseract.image_to_string(image)\\n\\nprint(text)\\n```\\n\\n### Example Analysis of the Provided Image\\n\\n1. **Text Detection**: Use Tesseract to extract the text \"Lexus\", \"L/Certified by Lexus\", \"Tap the screen to find the nearest Lexus dealership\".\\n2. **Object Detection**: Use TensorFlow or OpenCV to detect the camera lens and the hand icon.\\n3. **Color Detection**: Use OpenCV to identify the dominant colors in the image, such as black, white, and the colors on the camera lens.\\n\\nBy combining these tools and techniques, you can perform comprehensive image analysis on the provided image.', role='assistant', function_call=None, tool_calls=None))], created=1720767684, model='gpt-4o-2024-05-13', object='chat.completion', service_tier=None, system_fingerprint='fp_298125635f', usage=CompletionUsage(completion_tokens=752, prompt_tokens=495, total_tokens=1247))\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'ChatCompletion' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 63\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m calculate_gpt4o_cost(input_tokens, output_tokens, batch_api)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# response = {  # Your actual response object from OpenAI API\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m#     \"messages\": [\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m#     # ... other response details ...\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# }\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m cost \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_cost_from_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEstimated cost: $\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcost\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     66\u001b[0m cost_batch \u001b[38;5;241m=\u001b[39m calculate_cost_from_response(response, batch_api\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[9], line 44\u001b[0m, in \u001b[0;36mcalculate_cost_from_response\u001b[0;34m(response, batch_api)\u001b[0m\n\u001b[1;32m     41\u001b[0m encoding \u001b[38;5;241m=\u001b[39m tiktoken\u001b[38;5;241m.\u001b[39mencoding_for_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Use appropriate encoding\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Extract tokens from the response\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m input_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(msg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m msg \u001b[38;5;129;01min\u001b[39;00m \u001b[43mresponse\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m msg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     45\u001b[0m output_text \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     46\u001b[0m input_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(encoding\u001b[38;5;241m.\u001b[39mencode(input_text))\n",
      "\u001b[0;31mTypeError\u001b[0m: 'ChatCompletion' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def calculate_gpt4o_cost(input_tokens, output_tokens, batch_api=False):\n",
    "    \"\"\"\n",
    "    Calculates the cost of using GPT-4o based on the number of input and output tokens.\n",
    "\n",
    "    Args:\n",
    "        input_tokens (int): The number of input tokens.\n",
    "        output_tokens (int): The number of output tokens.\n",
    "        batch_api (bool): Whether or not to use Batch API pricing (default: False).\n",
    "\n",
    "    Returns:\n",
    "        float: The total cost in USD.\n",
    "    \"\"\"\n",
    "\n",
    "    input_price_per_million = 2.50 if batch_api else 5.00  # Adjust for Batch API\n",
    "    output_price_per_million = 7.50 if batch_api else 15.00  # Adjust for Batch API\n",
    "\n",
    "    # Convert token counts to millions\n",
    "    input_tokens_millions = input_tokens / 1_000_000\n",
    "    output_tokens_millions = output_tokens / 1_000_000\n",
    "\n",
    "    # Calculate costs\n",
    "    input_cost = input_tokens_millions * input_price_per_million\n",
    "    output_cost = output_tokens_millions * output_price_per_million\n",
    "    total_cost = input_cost + output_cost\n",
    "\n",
    "    return total_cost\n",
    "\n",
    "def calculate_cost_from_response(response, batch_api=False):\n",
    "    \"\"\"\n",
    "    Calculates the cost of a GPT-4o response based on its token usage.\n",
    "\n",
    "    Args:\n",
    "        response (ChatCompletion): The response object from the OpenAI API.\n",
    "        batch_api (bool): Whether or not to use Batch API pricing (default: False).\n",
    "\n",
    "    Returns:\n",
    "        float: The total cost in USD.\n",
    "    \"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-4\")  # Use appropriate encoding\n",
    "\n",
    "    # Extract tokens from the response\n",
    "    input_text = \"\".join(msg[\"content\"] for msg in response[\"messages\"] if msg[\"role\"] == \"user\")\n",
    "    output_text = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    input_tokens = len(encoding.encode(input_text))\n",
    "    output_tokens = len(encoding.encode(output_text))\n",
    "\n",
    "    return calculate_gpt4o_cost(input_tokens, output_tokens, batch_api)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# response = {  # Your actual response object from OpenAI API\n",
    "#     \"messages\": [\n",
    "#         {\"role\": \"user\", \"content\": \"What's in this image?\"},\n",
    "#     ],\n",
    "#     \"choices\": [\n",
    "#         {\"message\": {\"role\": \"assistant\", \"content\": \"...\"}}  # Your response content here\n",
    "#     ],\n",
    "#     # ... other response details ...\n",
    "# }\n",
    "\n",
    "cost = calculate_cost_from_response(response)\n",
    "print(f\"Estimated cost: ${cost:.2f}\")\n",
    "\n",
    "cost_batch = calculate_cost_from_response(response, batch_api=True)\n",
    "print(f\"Estimated cost with Batch API: ${cost_batch:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
